{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/06 16:59:56 WARN Utils: Your hostname, DESKTOP-TVDS9T6 resolves to a loopback address: 127.0.1.1; using 172.29.38.221 instead (on interface eth0)\n",
      "22/11/06 16:59:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/06 16:59:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ed/projects/dfender/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/ed/projects/dfender/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "X = pd.DataFrame(zip(range(100), range(100)), columns=['x','y'])\n",
    "X_dask = dd.from_pandas(X, npartitions=1)\n",
    "X_spark = spark.createDataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ed/projects/dfender/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/ed/projects/dfender/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|  x|pred|\n",
      "+---+----+\n",
      "|  0|{-2}|\n",
      "|  1|{-1}|\n",
      "|  2| {0}|\n",
      "|  3| {1}|\n",
      "|  4| {2}|\n",
      "|  5| {3}|\n",
      "|  6| {4}|\n",
      "|  7| {5}|\n",
      "|  8| {6}|\n",
      "|  9| {7}|\n",
      "| 10| {8}|\n",
      "| 11| {9}|\n",
      "| 12|{10}|\n",
      "| 13|{11}|\n",
      "| 14|{12}|\n",
      "| 15|{13}|\n",
      "| 16|{14}|\n",
      "| 17|{15}|\n",
      "| 18|{16}|\n",
      "| 19|{17}|\n",
      "+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dfender import distributable\n",
    "\n",
    "class Test:\n",
    "    @distributable(pd.Series)\n",
    "    def predict(self, X: pd.Series) -> pd.Series:\n",
    "        return X - 2\n",
    "        # return (pd.to_datetime('2000-01-01').to_period('M') + X).dt.to_timestamp()\n",
    "\n",
    "    @distributable((pd.Series, pd.DataFrame))\n",
    "    def rowmax(self, X: pd.DataFrame):\n",
    "        return X.max(axis=1)\n",
    "\n",
    "\n",
    "est = Test()\n",
    "est.predict(X['x'])\n",
    "est.predict(X_dask['x']).compute()\n",
    "\n",
    "X_spark.select([\n",
    "    \"x\",\n",
    "    est.predict(X_spark.select(['x'])).alias('pred')\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ed/projects/dfender/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/ed/projects/dfender/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(x=0, pred=Row(x=-2)),\n",
       " Row(x=1, pred=Row(x=-1)),\n",
       " Row(x=2, pred=Row(x=0)),\n",
       " Row(x=3, pred=Row(x=1)),\n",
       " Row(x=4, pred=Row(x=2)),\n",
       " Row(x=5, pred=Row(x=3)),\n",
       " Row(x=6, pred=Row(x=4)),\n",
       " Row(x=7, pred=Row(x=5)),\n",
       " Row(x=8, pred=Row(x=6)),\n",
       " Row(x=9, pred=Row(x=7)),\n",
       " Row(x=10, pred=Row(x=8)),\n",
       " Row(x=11, pred=Row(x=9)),\n",
       " Row(x=12, pred=Row(x=10)),\n",
       " Row(x=13, pred=Row(x=11)),\n",
       " Row(x=14, pred=Row(x=12)),\n",
       " Row(x=15, pred=Row(x=13)),\n",
       " Row(x=16, pred=Row(x=14)),\n",
       " Row(x=17, pred=Row(x=15)),\n",
       " Row(x=18, pred=Row(x=16)),\n",
       " Row(x=19, pred=Row(x=17)),\n",
       " Row(x=20, pred=Row(x=18)),\n",
       " Row(x=21, pred=Row(x=19)),\n",
       " Row(x=22, pred=Row(x=20)),\n",
       " Row(x=23, pred=Row(x=21)),\n",
       " Row(x=24, pred=Row(x=22)),\n",
       " Row(x=25, pred=Row(x=23)),\n",
       " Row(x=26, pred=Row(x=24)),\n",
       " Row(x=27, pred=Row(x=25)),\n",
       " Row(x=28, pred=Row(x=26)),\n",
       " Row(x=29, pred=Row(x=27)),\n",
       " Row(x=30, pred=Row(x=28)),\n",
       " Row(x=31, pred=Row(x=29)),\n",
       " Row(x=32, pred=Row(x=30)),\n",
       " Row(x=33, pred=Row(x=31)),\n",
       " Row(x=34, pred=Row(x=32)),\n",
       " Row(x=35, pred=Row(x=33)),\n",
       " Row(x=36, pred=Row(x=34)),\n",
       " Row(x=37, pred=Row(x=35)),\n",
       " Row(x=38, pred=Row(x=36)),\n",
       " Row(x=39, pred=Row(x=37)),\n",
       " Row(x=40, pred=Row(x=38)),\n",
       " Row(x=41, pred=Row(x=39)),\n",
       " Row(x=42, pred=Row(x=40)),\n",
       " Row(x=43, pred=Row(x=41)),\n",
       " Row(x=44, pred=Row(x=42)),\n",
       " Row(x=45, pred=Row(x=43)),\n",
       " Row(x=46, pred=Row(x=44)),\n",
       " Row(x=47, pred=Row(x=45)),\n",
       " Row(x=48, pred=Row(x=46)),\n",
       " Row(x=49, pred=Row(x=47)),\n",
       " Row(x=50, pred=Row(x=48)),\n",
       " Row(x=51, pred=Row(x=49)),\n",
       " Row(x=52, pred=Row(x=50)),\n",
       " Row(x=53, pred=Row(x=51)),\n",
       " Row(x=54, pred=Row(x=52)),\n",
       " Row(x=55, pred=Row(x=53)),\n",
       " Row(x=56, pred=Row(x=54)),\n",
       " Row(x=57, pred=Row(x=55)),\n",
       " Row(x=58, pred=Row(x=56)),\n",
       " Row(x=59, pred=Row(x=57)),\n",
       " Row(x=60, pred=Row(x=58)),\n",
       " Row(x=61, pred=Row(x=59)),\n",
       " Row(x=62, pred=Row(x=60)),\n",
       " Row(x=63, pred=Row(x=61)),\n",
       " Row(x=64, pred=Row(x=62)),\n",
       " Row(x=65, pred=Row(x=63)),\n",
       " Row(x=66, pred=Row(x=64)),\n",
       " Row(x=67, pred=Row(x=65)),\n",
       " Row(x=68, pred=Row(x=66)),\n",
       " Row(x=69, pred=Row(x=67)),\n",
       " Row(x=70, pred=Row(x=68)),\n",
       " Row(x=71, pred=Row(x=69)),\n",
       " Row(x=72, pred=Row(x=70)),\n",
       " Row(x=73, pred=Row(x=71)),\n",
       " Row(x=74, pred=Row(x=72)),\n",
       " Row(x=75, pred=Row(x=73)),\n",
       " Row(x=76, pred=Row(x=74)),\n",
       " Row(x=77, pred=Row(x=75)),\n",
       " Row(x=78, pred=Row(x=76)),\n",
       " Row(x=79, pred=Row(x=77)),\n",
       " Row(x=80, pred=Row(x=78)),\n",
       " Row(x=81, pred=Row(x=79)),\n",
       " Row(x=82, pred=Row(x=80)),\n",
       " Row(x=83, pred=Row(x=81)),\n",
       " Row(x=84, pred=Row(x=82)),\n",
       " Row(x=85, pred=Row(x=83)),\n",
       " Row(x=86, pred=Row(x=84)),\n",
       " Row(x=87, pred=Row(x=85)),\n",
       " Row(x=88, pred=Row(x=86)),\n",
       " Row(x=89, pred=Row(x=87)),\n",
       " Row(x=90, pred=Row(x=88)),\n",
       " Row(x=91, pred=Row(x=89)),\n",
       " Row(x=92, pred=Row(x=90)),\n",
       " Row(x=93, pred=Row(x=91)),\n",
       " Row(x=94, pred=Row(x=92)),\n",
       " Row(x=95, pred=Row(x=93)),\n",
       " Row(x=96, pred=Row(x=94)),\n",
       " Row(x=97, pred=Row(x=95)),\n",
       " Row(x=98, pred=Row(x=96)),\n",
       " Row(x=99, pred=Row(x=97))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_spark.select([\n",
    "    \"x\",\n",
    "    est.predict(X_spark.select(['x'])).alias('pred')\n",
    "]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(new=0),\n",
       " Row(new=1),\n",
       " Row(new=4),\n",
       " Row(new=9),\n",
       " Row(new=16),\n",
       " Row(new=25),\n",
       " Row(new=36),\n",
       " Row(new=49),\n",
       " Row(new=64),\n",
       " Row(new=81),\n",
       " Row(new=100),\n",
       " Row(new=121),\n",
       " Row(new=144),\n",
       " Row(new=169),\n",
       " Row(new=196),\n",
       " Row(new=225),\n",
       " Row(new=256),\n",
       " Row(new=289),\n",
       " Row(new=324),\n",
       " Row(new=361),\n",
       " Row(new=400),\n",
       " Row(new=441),\n",
       " Row(new=484),\n",
       " Row(new=529),\n",
       " Row(new=576),\n",
       " Row(new=625),\n",
       " Row(new=676),\n",
       " Row(new=729),\n",
       " Row(new=784),\n",
       " Row(new=841),\n",
       " Row(new=900),\n",
       " Row(new=961),\n",
       " Row(new=1024),\n",
       " Row(new=1089),\n",
       " Row(new=1156),\n",
       " Row(new=1225),\n",
       " Row(new=1296),\n",
       " Row(new=1369),\n",
       " Row(new=1444),\n",
       " Row(new=1521),\n",
       " Row(new=1600),\n",
       " Row(new=1681),\n",
       " Row(new=1764),\n",
       " Row(new=1849),\n",
       " Row(new=1936),\n",
       " Row(new=2025),\n",
       " Row(new=2116),\n",
       " Row(new=2209),\n",
       " Row(new=2304),\n",
       " Row(new=2401),\n",
       " Row(new=2500),\n",
       " Row(new=2601),\n",
       " Row(new=2704),\n",
       " Row(new=2809),\n",
       " Row(new=2916),\n",
       " Row(new=3025),\n",
       " Row(new=3136),\n",
       " Row(new=3249),\n",
       " Row(new=3364),\n",
       " Row(new=3481),\n",
       " Row(new=3600),\n",
       " Row(new=3721),\n",
       " Row(new=3844),\n",
       " Row(new=3969),\n",
       " Row(new=4096),\n",
       " Row(new=4225),\n",
       " Row(new=4356),\n",
       " Row(new=4489),\n",
       " Row(new=4624),\n",
       " Row(new=4761),\n",
       " Row(new=4900),\n",
       " Row(new=5041),\n",
       " Row(new=5184),\n",
       " Row(new=5329),\n",
       " Row(new=5476),\n",
       " Row(new=5625),\n",
       " Row(new=5776),\n",
       " Row(new=5929),\n",
       " Row(new=6084),\n",
       " Row(new=6241),\n",
       " Row(new=6400),\n",
       " Row(new=6561),\n",
       " Row(new=6724),\n",
       " Row(new=6889),\n",
       " Row(new=7056),\n",
       " Row(new=7225),\n",
       " Row(new=7396),\n",
       " Row(new=7569),\n",
       " Row(new=7744),\n",
       " Row(new=7921),\n",
       " Row(new=8100),\n",
       " Row(new=8281),\n",
       " Row(new=8464),\n",
       " Row(new=8649),\n",
       " Row(new=8836),\n",
       " Row(new=9025),\n",
       " Row(new=9216),\n",
       " Row(new=9409),\n",
       " Row(new=9604),\n",
       " Row(new=9801)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct, pandas_udf\n",
    "\n",
    "@pandas_udf(\"long\")\n",
    "def test(df):\n",
    "    return df.prod(axis=1)\n",
    "\n",
    "X_spark.select(test(struct(*X_spark.columns)).alias('new')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ed/projects/dfender/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/ed/projects/dfender/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Column<'func_(struct(x, x))'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.predict(X_spark.select(['x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'struct(x, y)'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "\n",
    "struct(*X_spark.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(test(x)=-2),\n",
       " Row(test(x)=-1),\n",
       " Row(test(x)=0),\n",
       " Row(test(x)=1),\n",
       " Row(test(x)=2),\n",
       " Row(test(x)=3),\n",
       " Row(test(x)=4),\n",
       " Row(test(x)=5),\n",
       " Row(test(x)=6),\n",
       " Row(test(x)=7),\n",
       " Row(test(x)=8),\n",
       " Row(test(x)=9),\n",
       " Row(test(x)=10),\n",
       " Row(test(x)=11),\n",
       " Row(test(x)=12),\n",
       " Row(test(x)=13),\n",
       " Row(test(x)=14),\n",
       " Row(test(x)=15),\n",
       " Row(test(x)=16),\n",
       " Row(test(x)=17),\n",
       " Row(test(x)=18),\n",
       " Row(test(x)=19),\n",
       " Row(test(x)=20),\n",
       " Row(test(x)=21),\n",
       " Row(test(x)=22),\n",
       " Row(test(x)=23),\n",
       " Row(test(x)=24),\n",
       " Row(test(x)=25),\n",
       " Row(test(x)=26),\n",
       " Row(test(x)=27),\n",
       " Row(test(x)=28),\n",
       " Row(test(x)=29),\n",
       " Row(test(x)=30),\n",
       " Row(test(x)=31),\n",
       " Row(test(x)=32),\n",
       " Row(test(x)=33),\n",
       " Row(test(x)=34),\n",
       " Row(test(x)=35),\n",
       " Row(test(x)=36),\n",
       " Row(test(x)=37),\n",
       " Row(test(x)=38),\n",
       " Row(test(x)=39),\n",
       " Row(test(x)=40),\n",
       " Row(test(x)=41),\n",
       " Row(test(x)=42),\n",
       " Row(test(x)=43),\n",
       " Row(test(x)=44),\n",
       " Row(test(x)=45),\n",
       " Row(test(x)=46),\n",
       " Row(test(x)=47),\n",
       " Row(test(x)=48),\n",
       " Row(test(x)=49),\n",
       " Row(test(x)=50),\n",
       " Row(test(x)=51),\n",
       " Row(test(x)=52),\n",
       " Row(test(x)=53),\n",
       " Row(test(x)=54),\n",
       " Row(test(x)=55),\n",
       " Row(test(x)=56),\n",
       " Row(test(x)=57),\n",
       " Row(test(x)=58),\n",
       " Row(test(x)=59),\n",
       " Row(test(x)=60),\n",
       " Row(test(x)=61),\n",
       " Row(test(x)=62),\n",
       " Row(test(x)=63),\n",
       " Row(test(x)=64),\n",
       " Row(test(x)=65),\n",
       " Row(test(x)=66),\n",
       " Row(test(x)=67),\n",
       " Row(test(x)=68),\n",
       " Row(test(x)=69),\n",
       " Row(test(x)=70),\n",
       " Row(test(x)=71),\n",
       " Row(test(x)=72),\n",
       " Row(test(x)=73),\n",
       " Row(test(x)=74),\n",
       " Row(test(x)=75),\n",
       " Row(test(x)=76),\n",
       " Row(test(x)=77),\n",
       " Row(test(x)=78),\n",
       " Row(test(x)=79),\n",
       " Row(test(x)=80),\n",
       " Row(test(x)=81),\n",
       " Row(test(x)=82),\n",
       " Row(test(x)=83),\n",
       " Row(test(x)=84),\n",
       " Row(test(x)=85),\n",
       " Row(test(x)=86),\n",
       " Row(test(x)=87),\n",
       " Row(test(x)=88),\n",
       " Row(test(x)=89),\n",
       " Row(test(x)=90),\n",
       " Row(test(x)=91),\n",
       " Row(test(x)=92),\n",
       " Row(test(x)=93),\n",
       " Row(test(x)=94),\n",
       " Row(test(x)=95),\n",
       " Row(test(x)=96),\n",
       " Row(test(x)=97)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf(\"bigint\")\n",
    "def test(df):\n",
    "    return df - 2\n",
    "\n",
    "X_spark.select(test(\"x\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ed/projects/dfender/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/ed/projects/dfender/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m est\u001b[39m.\u001b[39;49mpredict(X_spark\u001b[39m.\u001b[39;49mselect([\u001b[39m'\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m'\u001b[39;49m]))\u001b[39m.\u001b[39;49mcollect()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "est.predict(X_spark.select(['x'])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(y=0),\n",
       " Row(y=0),\n",
       " Row(y=1),\n",
       " Row(y=1),\n",
       " Row(y=2),\n",
       " Row(y=2),\n",
       " Row(y=3),\n",
       " Row(y=3),\n",
       " Row(y=4),\n",
       " Row(y=4),\n",
       " Row(y=5),\n",
       " Row(y=5),\n",
       " Row(y=6),\n",
       " Row(y=6),\n",
       " Row(y=7),\n",
       " Row(y=7),\n",
       " Row(y=8),\n",
       " Row(y=8),\n",
       " Row(y=9),\n",
       " Row(y=9),\n",
       " Row(y=10),\n",
       " Row(y=10),\n",
       " Row(y=11),\n",
       " Row(y=11),\n",
       " Row(y=12),\n",
       " Row(y=12),\n",
       " Row(y=13),\n",
       " Row(y=13),\n",
       " Row(y=14),\n",
       " Row(y=14),\n",
       " Row(y=15),\n",
       " Row(y=15),\n",
       " Row(y=16),\n",
       " Row(y=16),\n",
       " Row(y=17),\n",
       " Row(y=17),\n",
       " Row(y=18),\n",
       " Row(y=18),\n",
       " Row(y=19),\n",
       " Row(y=19),\n",
       " Row(y=20),\n",
       " Row(y=20),\n",
       " Row(y=21),\n",
       " Row(y=21),\n",
       " Row(y=22),\n",
       " Row(y=22),\n",
       " Row(y=23),\n",
       " Row(y=23),\n",
       " Row(y=24),\n",
       " Row(y=24),\n",
       " Row(y=25),\n",
       " Row(y=25),\n",
       " Row(y=26),\n",
       " Row(y=26),\n",
       " Row(y=27),\n",
       " Row(y=27),\n",
       " Row(y=28),\n",
       " Row(y=28),\n",
       " Row(y=29),\n",
       " Row(y=29),\n",
       " Row(y=30),\n",
       " Row(y=30),\n",
       " Row(y=31),\n",
       " Row(y=31),\n",
       " Row(y=32),\n",
       " Row(y=32),\n",
       " Row(y=33),\n",
       " Row(y=33),\n",
       " Row(y=34),\n",
       " Row(y=34),\n",
       " Row(y=35),\n",
       " Row(y=35),\n",
       " Row(y=36),\n",
       " Row(y=36),\n",
       " Row(y=37),\n",
       " Row(y=37),\n",
       " Row(y=38),\n",
       " Row(y=38),\n",
       " Row(y=39),\n",
       " Row(y=39),\n",
       " Row(y=40),\n",
       " Row(y=40),\n",
       " Row(y=41),\n",
       " Row(y=41),\n",
       " Row(y=42),\n",
       " Row(y=42),\n",
       " Row(y=43),\n",
       " Row(y=43),\n",
       " Row(y=44),\n",
       " Row(y=44),\n",
       " Row(y=45),\n",
       " Row(y=45),\n",
       " Row(y=46),\n",
       " Row(y=46),\n",
       " Row(y=47),\n",
       " Row(y=47),\n",
       " Row(y=48),\n",
       " Row(y=48),\n",
       " Row(y=49),\n",
       " Row(y=49)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "@pandas_udf(\"long\")\n",
    "def test(df: pd.Series) -> pd.Series:\n",
    "    return (df / 2)\n",
    "\n",
    "X_spark.select([test(\"x\").alias('y')]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ed/projects/dfender/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/ed/projects/dfender/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x bigint\n",
      "22/11/06 16:35:40 ERROR Executor: Exception in task 16.0 in stage 1.0 (TID 40)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_28773/557874211.py\", line 6, in predict\n",
      "TypeError: unsupported operand type(s) for -: 'map' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/11/06 16:35:40 ERROR Executor: Exception in task 11.0 in stage 1.0 (TID 35)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_28773/557874211.py\", line 6, in predict\n",
      "TypeError: unsupported operand type(s) for -: 'map' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/11/06 16:35:40 ERROR Executor: Exception in task 12.0 in stage 1.0 (TID 36)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_28773/557874211.py\", line 6, in predict\n",
      "TypeError: unsupported operand type(s) for -: 'map' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/11/06 16:35:40 ERROR Executor: Exception in task 10.0 in stage 1.0 (TID 34)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_28773/557874211.py\", line 6, in predict\n",
      "TypeError: unsupported operand type(s) for -: 'map' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/11/06 16:35:40 ERROR Executor: Exception in task 14.0 in stage 1.0 (TID 38)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_28773/557874211.py\", line 6, in predict\n",
      "TypeError: unsupported operand type(s) for -: 'map' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/11/06 16:35:40 ERROR Executor: Exception in task 6.0 in stage 1.0 (TID 30)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_28773/557874211.py\", line 6, in predict\n",
      "TypeError: unsupported operand type(s) for -: 'map' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/11/06 16:35:40 ERROR Executor: Exception in task 18.0 in stage 1.0 (TID 42)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_28773/557874211.py\", line 6, in predict\n",
      "TypeError: unsupported operand type(s) for -: 'map' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/11/06 16:35:40 ERROR Executor: Exception in task 9.0 in stage 1.0 (TID 33)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_28773/557874211.py\", line 6, in predict\n",
      "TypeError: unsupported operand type(s) for -: 'map' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/11/06 16:35:40 WARN TaskSetManager: Lost task 12.0 in stage 1.0 (TID 36) (172.29.38.221 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_28773/557874211.py\", line 6, in predict\n",
      "TypeError: unsupported operand type(s) for -: 'map' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/11/06 16:35:40 ERROR Executor: Exception in task 13.0 in stage 1.0 (TID 37)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_28773/557874211.py\", line 6, in predict\n",
      "TypeError: unsupported operand type(s) for -: 'map' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/11/06 16:35:40 ERROR TaskSetManager: Task 12 in stage 1.0 failed 1 times; aborting job\n",
      "22/11/06 16:35:40 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 25) (172.29.38.221 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/06 16:35:40 WARN TaskSetManager: Lost task 17.0 in stage 1.0 (TID 41) (172.29.38.221 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/06 16:35:40 WARN TaskSetManager: Lost task 5.0 in stage 1.0 (TID 29) (172.29.38.221 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/06 16:35:40 WARN TaskSetManager: Lost task 3.0 in stage 1.0 (TID 27) (172.29.38.221 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/06 16:35:40 WARN TaskSetManager: Lost task 22.0 in stage 1.0 (TID 46) (172.29.38.221 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/06 16:35:40 WARN TaskSetManager: Lost task 15.0 in stage 1.0 (TID 39) (172.29.38.221 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/06 16:35:40 WARN TaskSetManager: Lost task 8.0 in stage 1.0 (TID 32) (172.29.38.221 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/06 16:35:40 WARN TaskSetManager: Lost task 23.0 in stage 1.0 (TID 47) (172.29.38.221 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/06 16:35:40 WARN TaskSetManager: Lost task 7.0 in stage 1.0 (TID 31) (172.29.38.221 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/06 16:35:40 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 24) (172.29.38.221 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/06 16:35:40 WARN TaskSetManager: Lost task 19.0 in stage 1.0 (TID 43) (172.29.38.221 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/06 16:35:40 WARN TaskSetManager: Lost task 4.0 in stage 1.0 (TID 28) (172.29.38.221 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/06 16:35:40 WARN TaskSetManager: Lost task 2.0 in stage 1.0 (TID 26) (172.29.38.221 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/06 16:35:40 WARN TaskSetManager: Lost task 21.0 in stage 1.0 (TID 45) (172.29.38.221 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/06 16:35:40 WARN TaskSetManager: Lost task 20.0 in stage 1.0 (TID 44) (172.29.38.221 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_28773/557874211.py\", line 6, in predict\nTypeError: unsupported operand type(s) for -: 'map' and 'int'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m est\u001b[39m.\u001b[39;49mpredict(X_spark\u001b[39m.\u001b[39;49mselect([\u001b[39m'\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m'\u001b[39;49m]))\u001b[39m.\u001b[39;49mcollect()\n",
      "File \u001b[0;32m~/projects/dfender/.venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[39m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \n\u001b[1;32m    809\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc):\n\u001b[0;32m--> 817\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcollectToPython()\n\u001b[1;32m    818\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/projects/dfender/.venv/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/projects/dfender/.venv/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_28773/557874211.py\", line 6, in predict\nTypeError: unsupported operand type(s) for -: 'map' and 'int'\n"
     ]
    }
   ],
   "source": [
    "est.predict(X_spark.select(['x'])).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bd7d7fb9277fd46b659b9902a765d4b7625d610e42224668afbeffa62bf2abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
